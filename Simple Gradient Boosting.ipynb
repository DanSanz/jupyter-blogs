{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In this article, I want to introduce you to one of the key algorithms behind successful learning to rank implementations: gradient boosting. Gradient boosting is the \"brain\" behind learning to rank. It's the core idea behind LambdaMART. In this article I want to walk you through why it's such a powerful machine-learning algorithm for search relevance in particular. You'll see it's ability to capture subtle, non-linear with some level of interpretability helps make gradient boosting really powerful solution for search, personalization, and recommendation systems.\n",
    "\n",
    "## Learning to Rank as a Regression Problem\n",
    "To introduce you to gradient boosting, we're NOT going to look at traditional learning to rank beyond this section. So I want to map learning to rank, as you might be familiar from [previous articles](http://opensourceconnections.com/blog/2017/02/24/what-is-learning-to-rank/) and [documentation](https://github.com/o19s/elasticsearch-learning-to-rank#building-a-learning-to-rank-system-with-elasticsearch) to a more general problem: regression. *Regression* trains a model to map a set of numerical features to a predicted numerical value. \n",
    "\n",
    "For example, what if you wanted to be able to predict a company's profit? You might have, on hand, historical data about public corporations including number of employees, stock market price, revenue, cash on hand, etc. Given data you know about existing companies, your model could be trained to predict profit as a function of these variables (or a subset thereof). For a new company you could use your function to arrive at a prediction of the company's profit.\n",
    "\n",
    "Just the same, learning to rank can be a regression problem. You have on hand a series of *judgments* that grade how relevant a document is for a query. Our relevance grades could range from A to F. More commonly they range from 0 (not at all relevant) to 4 (exactly relevant). If we just consider a keyword search to be a query, this become, as an example:\n",
    "\n",
    "\n",
    "    grade,movie,keywordquery\n",
    "    4,Rambo,rambo\n",
    "    0,Turner and Hootch,rambo\n",
    "    4,First Blood,rambo\n",
    "    1,Rocky,rambo\n",
    "    ...\n",
    "    \n",
    "    \n",
    "Learning to Rank becomes a regression problem when you build a model to predict the *grade* as a function of ranking-time *signals.* Recall from [Relevant Search](http://manning.com/books/relevant-search) we term signals to mean any measurement about the relationship between the query and a document. Often signals are *query-dependent* - that is they result by taking some measurement of how a keyword (or other part of the query) relates to the document. Other times they are query or document-only, such as publication date, or whether a \"company name\" could be extracted from the query using NLP method.\n",
    "\n",
    "In other words, let's consider the movie example above. You might have 2 query-dependent signals you suspect could help predict relevance:\n",
    "\n",
    "1. How many times a search keyword occurs in the **title** field\n",
    "2. How many times a search keyword occurs in the **overview** field\n",
    "\n",
    "Augmenting the *judgments* above, you might arrive at a training like below:\n",
    "\n",
    "    ```\n",
    "    4,1,1\n",
    "    0,0,0\n",
    "    4,0,3\n",
    "    1,0,1\n",
    "    ```\n",
    "    \n",
    "You can apply a regression process (really any regression process, including linear regression), to predict the first column using the other columns. You can build such a system on top of an existing search engine like [Solr](https://cwiki.apache.org/confluence/display/solr/Learning+To+Rank) or [Elasticsearch](http://opensourceconnections.com/blog/2017/02/14/elasticsearch-learning-to-rank/).\n",
    "\n",
    "Learning to Rank comes with a few extra complications we'll save for a future article\n",
    "\n",
    "1. Examples are provided in groups, grouped by queries. A document can be a grade 4 for query \"rambo\" and a 0 for query \"beauty and the beast.\"\n",
    "2. How do you arrive at these \"grades\"? \n",
    "3. How does all this work in practice?\n",
    "\n",
    "Yes yes, but for now I want to look at a specific *kind* of regression. It's the magic under learning to rank and I want to teach you all about it.\n",
    "\n",
    "\n",
    "\n",
    "## What *kind* of regression works best for Learning to Rank?\n",
    "\n",
    "If you've learned any statistics, you may be familiar with Linear Regression. *Linear Regression* defines the regression problem as a simple linear function. For example, if in learning to rank we called the first signal above (how many times a search keyword occurs it the **title** field) as `t` and the second signal above (the same for the **overview** field) as `o`, our model might be able to generate a function `s` to score our relevance as follows:\n",
    "\n",
    "    s(t, o) = c0 + c1 * t + c2 * o\n",
    "    \n",
    "\n",
    "We can estimate the best fit coefficients `c0, c1, c2...` that predict our training data using a procedure known as [least squares fitting](http://mathworld.wolfram.com/LeastSquaresFitting.html). We won't cover that here, but the gist is we can find the `c0, c1, c2, ...` that minimize the error between the actual grade, `g` and the prediction `s(t,o)`. \n",
    "\n",
    "You can get fancier with linear regression, including deciding there's really a third ranking signal, `a` which we can define as `t*o`. Or another signal called t2, which could be in reality `t^2` or `log(t)` or whatever formulation you suspect could help best predict relevance.\n",
    "\n",
    "There's a deeper art to of designing, testing ,and evaluating models of any flavor, I heartily recommend [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) if you'd like to learn more.\n",
    "\n",
    "\n",
    "## Playing with Linear Regression in sklearn\n",
    "\n",
    "To give you a taste, Python's [sklearn](http://scikit-learn.org/stable/) family of libraries is a convenient way to play with regression. If we want to try out the simple learning to rank training set above for linear regression, we can express the relevance grade's we're trying to predict as `S`, and the signals we feel will predict that score as `X`:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sklearn.tree",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-86e389e5a3ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmath\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sklearn.tree"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegressor\n",
    "from math import sin\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "numFeatures = 10\n",
    "numDatapoints = 1000\n",
    "\n",
    "\n",
    "# Generate 100 samples with 10 random features\n",
    "X = np.random.random(numFeatures*numDatapoints).reshape(numDatapoints,numFeatures)\n",
    "Y = np.arctan(X).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Generate 100 random values\n",
    "Y = np.random.random(numDatapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_split=1e-07,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "           splitter='best')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1, perform a regression over the X & Y values\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.n_features=10\n",
    "tree.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5491898320864383e-08"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2. Next we make predictions using X, and see how far we are from Y\n",
    "\n",
    "Ypredicted = tree.predict(X)\n",
    "np.power(Y - Ypredicted, 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.243293215375147"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait, what, error is near 0! We've perfectly predicted the training data.\n",
    "# This isn't surprising. But it's likely an example of overfitting. Let's train on all but the \n",
    "# first 10 data points and then recalculate the *test error*\n",
    "Xtrain = X[100:]\n",
    "Xtest = X[:100]\n",
    "Ytrain = Y[100:]\n",
    "Ytest = Y[:100]\n",
    "tree.fit(Xtrain, Ytrain)\n",
    "Ypredicted = tree.predict(X)\n",
    "np.power(Y - Ypredicted, 2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.09914091,  0.31433702,  0.34429422,  0.55603666,  0.83777067,\n",
       "        0.38102517,  0.85519233,  0.8633129 ,  0.50484428,  0.76361867])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5206042 ,  0.45071729,  0.94032958,  0.68446533,  0.70176207,\n",
       "         0.75528753,  0.72025058,  0.22522763,  0.18514977,  0.84070608],\n",
       "       [ 0.09914091,  0.31433702,  0.34429422,  0.55603666,  0.83777067,\n",
       "         0.38102517,  0.85519233,  0.8633129 ,  0.50484428,  0.76361867],\n",
       "       [ 0.41816377,  0.53497742,  0.83310324,  0.05345249,  0.39873131,\n",
       "         0.11529311,  0.56652605,  0.04872523,  0.42650035,  0.41389597],\n",
       "       [ 0.32663508,  0.30300935,  0.57256465,  0.45474402,  0.37723288,\n",
       "         0.63898176,  0.25567442,  0.14961114,  0.73494424,  0.29039654],\n",
       "       [ 0.27869052,  0.07218927,  0.96985285,  0.65412681,  0.10789258,\n",
       "         0.10191945,  0.4291287 ,  0.61700272,  0.64590024,  0.07351153],\n",
       "       [ 0.48881607,  0.37273356,  0.16319504,  0.83005925,  0.67285752,\n",
       "         0.0910396 ,  0.42477796,  0.70907164,  0.04713346,  0.86128398],\n",
       "       [ 0.67287971,  0.14537552,  0.68258379,  0.21359162,  0.66374476,\n",
       "         0.57542011,  0.35296409,  0.22235011,  0.84359009,  0.4035306 ],\n",
       "       [ 0.66461191,  0.08489072,  0.67441646,  0.45016385,  0.67109611,\n",
       "         0.81942755,  0.32847737,  0.81201275,  0.73471927,  0.50587024],\n",
       "       [ 0.61299257,  0.27785954,  0.46223594,  0.38375412,  0.32318435,\n",
       "         0.91253534,  0.2185539 ,  0.41688906,  0.76961612,  0.1739477 ],\n",
       "       [ 0.2687397 ,  0.67840911,  0.36154367,  0.34180374,  0.20738571,\n",
       "         0.71217539,  0.91562314,  0.58115455,  0.07305912,  0.23084397]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
